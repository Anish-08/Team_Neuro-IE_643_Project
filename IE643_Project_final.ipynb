{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DsnDKzOpjiZe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import os\n",
        "import os.path\n",
        "from collections import OrderedDict\n",
        "\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import time\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JjVav18PjsIq"
      },
      "outputs": [],
      "source": [
        "## Define AlexNet model\n",
        "def compute_conv_output_size(Lin,kernel_size,stride=1,padding=0,dilation=1):\n",
        "    return int(np.floor((Lin+2*padding-dilation*(kernel_size-1)-1)/float(stride)+1))\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self,taskcla):\n",
        "        self.fbegin = 15\n",
        "        self.klen = 3\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.act=OrderedDict()\n",
        "        self.map =[]\n",
        "        self.ksize=[]\n",
        "        self.in_channel =[]\n",
        "        self.map.append(32)\n",
        "        self.conv1 = nn.Conv2d(3, 64, 4, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64, track_running_stats=False)\n",
        "        s=compute_conv_output_size(32,4)\n",
        "        s=s//2\n",
        "        self.ksize.append(4)\n",
        "        self.in_channel.append(3)\n",
        "        self.map.append(s)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(128, track_running_stats=False)\n",
        "        s=compute_conv_output_size(s,3)\n",
        "        s=s//2\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(64)\n",
        "        self.map.append(s)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 2, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(256, track_running_stats=False)\n",
        "        s=compute_conv_output_size(s,2)\n",
        "        s=s//2\n",
        "        self.smid=s\n",
        "        self.ksize.append(2)\n",
        "        self.in_channel.append(128)\n",
        "        self.map.append(256*self.smid*self.smid)\n",
        "        self.maxpool=torch.nn.MaxPool2d(2)\n",
        "        self.relu=torch.nn.ReLU()\n",
        "        self.drop1=torch.nn.Dropout(0.2)\n",
        "        self.drop2=torch.nn.Dropout(0.5)\n",
        "\n",
        "        self.fc1 = nn.Linear(256*self.smid*self.smid,2048, bias=False)\n",
        "        self.bn4 = nn.BatchNorm1d(2048, track_running_stats=False)\n",
        "        self.fc2 = nn.Linear(2048,2048, bias=False)\n",
        "        self.bn5 = nn.BatchNorm1d(2048, track_running_stats=False)\n",
        "        self.map.extend([2048])\n",
        "\n",
        "        self.taskcla = taskcla\n",
        "        self.fc3=torch.nn.ModuleList()\n",
        "        for t,n in self.taskcla:\n",
        "            self.fc3.append(torch.nn.Linear(2048,n,bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bsz = deepcopy(x.size(0))\n",
        "        self.act['conv1']=x\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(self.drop1(self.relu(self.bn1(x))))\n",
        "\n",
        "        self.act['conv2']=x\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool(self.drop1(self.relu(self.bn2(x))))\n",
        "\n",
        "\n",
        "        self.act['conv3']=x\n",
        "        x = self.conv3(x)\n",
        "        x = self.maxpool(self.drop2(self.relu(self.bn3(x))))\n",
        "\n",
        "\n",
        "        x=x.view(bsz,-1)\n",
        "        self.act['fc1']=x\n",
        "        x = self.fc1(x)\n",
        "        x = self.drop2(self.relu(self.bn4(x)))\n",
        "\n",
        "\n",
        "        self.act['fc2']=x\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(self.relu(self.bn5(x)))\n",
        "\n",
        "\n",
        "        y=[]\n",
        "        for t,i in self.taskcla:\n",
        "            y.append(self.fc3[t](x))\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xhwcHnX9n8X8"
      },
      "outputs": [],
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self,taskcla):\n",
        "        self.fbegin = 4\n",
        "        self.klen = 2\n",
        "        super(LeNet, self).__init__()\n",
        "        self.act=OrderedDict()\n",
        "        self.map =[]\n",
        "        self.ksize=[]\n",
        "        self.in_channel =[]\n",
        "\n",
        "        self.map.append(32)\n",
        "        self.conv1 = nn.Conv2d(3, 20, 5, bias=False, padding=2)\n",
        "\n",
        "        s=compute_conv_output_size(32,5,1,2)\n",
        "        s=compute_conv_output_size(s,3,2,1)\n",
        "        self.ksize.append(5)\n",
        "        self.in_channel.append(3)\n",
        "        self.map.append(s)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, bias=False, padding=2)\n",
        "\n",
        "        s=compute_conv_output_size(s,5,1,2)\n",
        "        s=compute_conv_output_size(s,3,2,1)\n",
        "        self.ksize.append(5)\n",
        "        self.in_channel.append(20)\n",
        "        self.smid=s\n",
        "        self.map.append(50*self.smid*self.smid)\n",
        "        self.maxpool=torch.nn.MaxPool2d(3,2,padding=1)\n",
        "        self.relu=torch.nn.ReLU()\n",
        "        self.drop1=torch.nn.Dropout(0)\n",
        "        self.drop2=torch.nn.Dropout(0)\n",
        "        self.lrn = torch.nn.LocalResponseNorm(4,0.001/9.0,0.75,1)\n",
        "\n",
        "        self.fc1 = nn.Linear(50*self.smid*self.smid,800, bias=False)\n",
        "        self.fc2 = nn.Linear(800,500, bias=False)\n",
        "        self.map.extend([800])\n",
        "\n",
        "        self.taskcla = taskcla\n",
        "        self.fc3=torch.nn.ModuleList()\n",
        "        for t,n in self.taskcla:\n",
        "            self.fc3.append(torch.nn.Linear(500,n,bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bsz = deepcopy(x.size(0))\n",
        "        self.act['conv1']=x\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(self.drop1(self.lrn(self.relu(x))))\n",
        "\n",
        "        self.act['conv2']=x\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool(self.drop1(self.lrn (self.relu(x))))\n",
        "\n",
        "        x=x.reshape(bsz,-1)\n",
        "        self.act['fc1']=x\n",
        "        x = self.fc1(x)\n",
        "        x = self.drop2(self.relu(x))\n",
        "\n",
        "        self.act['fc2']=x\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop2(self.relu(x))\n",
        "\n",
        "        y=[]\n",
        "        for t,i in self.taskcla:\n",
        "            y.append(self.fc3[t](x))\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8mCbztHWP7AQ"
      },
      "outputs": [],
      "source": [
        "class VGG16(nn.Module):\n",
        "    def __init__(self, taskcla):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.ksize = []\n",
        "        self.in_channel = []\n",
        "        self.map = []\n",
        "\n",
        "        self.map.append(32)\n",
        "        self.act=OrderedDict()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(3)\n",
        "        s=compute_conv_output_size(32,3,padding=1)\n",
        "        self.map.append(s)\n",
        "\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(64)\n",
        "        s=compute_conv_output_size(s,3,padding=1)\n",
        "        s=s//2\n",
        "        self.map.append(s)\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(64)\n",
        "        s=compute_conv_output_size(s,3,padding=1)\n",
        "        self.map.append(s)\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(128)\n",
        "        s=compute_conv_output_size(s,3,padding=1)\n",
        "        s=s//2\n",
        "        self.map.append(s)\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(128)\n",
        "        s=compute_conv_output_size(s,3,padding=1)\n",
        "        self.map.append(s)\n",
        "\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU())\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(256)\n",
        "        s=compute_conv_output_size(s,3,padding=1)\n",
        "        self.map.append(s)\n",
        "\n",
        "\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride = 2))\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(256)\n",
        "        s=compute_conv_output_size(s,3,padding=1)\n",
        "        s=s//2\n",
        "        self.map.append(s)\n",
        "\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU())\n",
        "        self.ksize.append(3)\n",
        "        self.in_channel.append(256)\n",
        "        s=compute_conv_output_size(s,3,padding=1)\n",
        "\n",
        "\n",
        "        self.klen = len(self.ksize)\n",
        "        self.fbegin = 35\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512*s*s, 4096),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.map.append(512*s*s)\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.map.append(4096)\n",
        "        self.taskcla = taskcla\n",
        "        self.fc2=torch.nn.ModuleList()\n",
        "        for t,n in self.taskcla:\n",
        "            self.fc2.append(torch.nn.Linear(4096,n,bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(x.shape)\n",
        "        self.act['conv1']=x\n",
        "        out = self.layer1(x)\n",
        "        #print(out.shape)\n",
        "        self.act['conv2']=out\n",
        "        out = self.layer2(out)\n",
        "        #print(out.shape)\n",
        "        self.act['conv3']=out\n",
        "        out = self.layer3(out)\n",
        "        #print(out.shape)\n",
        "        self.act['conv4']=out\n",
        "        out = self.layer4(out)\n",
        "        #print(out.shape)\n",
        "        self.act['conv5']=out\n",
        "        out = self.layer5(out)\n",
        "        #print(out.shape)\n",
        "        self.act['conv6']=out\n",
        "        out = self.layer6(out)\n",
        "        #print(out.shape)\n",
        "        self.act['conv7']=out\n",
        "        out = self.layer7(out)\n",
        "        #print(out.shape)\n",
        "        self.act['conv8']=out\n",
        "        out = self.layer8(out)\n",
        "        #print(out.shape)\n",
        "\n",
        "\n",
        "        #out = self.layer1(out)\n",
        "\n",
        "        #print(self.map)\n",
        "\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        self.act['fc']=out\n",
        "        out = self.fc(out)\n",
        "        self.act['fc1']=out\n",
        "        out = self.fc1(out)\n",
        "        y=[]\n",
        "        for t,i in self.taskcla:\n",
        "            y.append(self.fc2[t](out))\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wP1jqvxejz0w"
      },
      "outputs": [],
      "source": [
        "def get_model(model):\n",
        "    return deepcopy(model.state_dict())\n",
        "\n",
        "def set_model_(model,state_dict):\n",
        "    model.load_state_dict(deepcopy(state_dict))\n",
        "    return\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        if (epoch ==1):\n",
        "            param_group['lr']=args.lr\n",
        "        else:\n",
        "            param_group['lr'] /= args.lr_factor\n",
        "\n",
        "def train(args, model, device, x,y, optimizer,criterion, task_id):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    print(device)\n",
        "    model.train()\n",
        "    r=np.arange(x.size(0))\n",
        "    np.random.shuffle(r)\n",
        "    r=torch.LongTensor(r).to(device)\n",
        "    # Loop batches\n",
        "    for i in range(0,len(r),args.batch_size_train):\n",
        "        if i+args.batch_size_train<=len(r): b=r[i:i+args.batch_size_train]\n",
        "        else: b=r[i:]\n",
        "        data = x[b]\n",
        "        data, target = data.to(device), y[b].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output[task_id], target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def train_projected(args,model,device,x,y,optimizer,criterion,feature_mat,task_id):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    model.train()\n",
        "    r=np.arange(x.size(0))\n",
        "    np.random.shuffle(r)\n",
        "    r=torch.LongTensor(r).to(device)\n",
        "    # Loop batches\n",
        "    for i in range(0,len(r),args.batch_size_train):\n",
        "        if i+args.batch_size_train<=len(r): b=r[i:i+args.batch_size_train]\n",
        "        else: b=r[i:]\n",
        "        data = x[b]\n",
        "        data, target = data.to(device), y[b].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output[task_id], target)\n",
        "        loss.backward()\n",
        "        # Gradient Projections\n",
        "        kk = 0\n",
        "        for k, (m,params) in enumerate(model.named_parameters()):\n",
        "            if k<model.fbegin and len(params.size())!=1:\n",
        "                sz =  params.grad.data.size(0)\n",
        "                params.grad.data = params.grad.data - torch.mm(params.grad.data.view(sz,-1),\\\n",
        "                                                        feature_mat[kk]).view(params.size())\n",
        "                kk +=1\n",
        "            elif (k<model.fbegin and len(params.size())==1) and task_id !=0 :\n",
        "                params.grad.data.fill_(0)\n",
        "\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "def test(args, model, device, x, y, criterion, task_id):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_num = 0\n",
        "    correct = 0\n",
        "    r=np.arange(x.size(0))\n",
        "    np.random.shuffle(r)\n",
        "    r=torch.LongTensor(r).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Loop batches\n",
        "        for i in range(0,len(r),args.batch_size_test):\n",
        "            if i+args.batch_size_test<=len(r): b=r[i:i+args.batch_size_test]\n",
        "            else: b=r[i:]\n",
        "            data = x[b]\n",
        "            data, target = data.to(device), y[b].to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output[task_id], target)\n",
        "            pred = output[task_id].argmax(dim=1, keepdim=True)\n",
        "\n",
        "            correct    += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total_loss += loss.data.cpu().numpy().item()*len(b)\n",
        "            total_num  += len(b)\n",
        "\n",
        "    acc = 100. * correct / total_num\n",
        "    final_loss = total_loss / total_num\n",
        "    return final_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IcGQu48Qj5-K"
      },
      "outputs": [],
      "source": [
        "def get_representation_matrix (net, device, x, y=None):\n",
        "    x = x.to(device)\n",
        "    # Collect activations by forward pass\n",
        "    r=np.arange(x.size(0))\n",
        "    np.random.shuffle(r)\n",
        "    r=torch.LongTensor(r).to(device)\n",
        "    b=r[0:125] # Take 125 random samples\n",
        "    example_data = x[b]\n",
        "    example_data = example_data.to(device)\n",
        "    example_out  = net(example_data)\n",
        "\n",
        "    #batch_list=[2*12,100,100,125,125,125,125,125,125,125,125,125,125,125,125,125]\n",
        "    batch_list = 15*[20]\n",
        "    mat_list=[]\n",
        "    act_key=list(net.act.keys())\n",
        "\n",
        "    #print(\"MAP: \")\n",
        "    #print(net.map)\n",
        "    #print(\"ksize\")\n",
        "    #print(net.ksize)\n",
        "    #print(\"in-c\")\n",
        "    #print(net.in_channel)\n",
        "\n",
        "    #print(\"ACT keys:\")\n",
        "    #print(act_key)\n",
        "\n",
        "    #print(len(net.map))\n",
        "    #print(len(act_key))\n",
        "\n",
        "    for i in range(len(net.map)):\n",
        "        bsz=batch_list[i]\n",
        "        k=0\n",
        "        if i<net.klen:\n",
        "            ksz= net.ksize[i]\n",
        "            s=compute_conv_output_size(net.map[i],net.ksize[i])\n",
        "\n",
        "            mat = np.zeros((net.ksize[i]*net.ksize[i]*net.in_channel[i],s*s*bsz))\n",
        "            act = net.act[act_key[i]].detach().cpu().numpy()\n",
        "            #print(f\"s is {s}\")\n",
        "            #print(f\"ksz is {ksz}\")\n",
        "            ##print(f\"shape of mat is {mat.shape}\")\n",
        "            #print(f\"shape of act is {act.shape}\")\n",
        "            #print(act.shape)\n",
        "            #print(net.in_channel[i])\n",
        "            #print(f\"done with {i}\")\n",
        "            for kk in range(bsz):\n",
        "                for ii in range(s):\n",
        "                    for jj in range(s):\n",
        "                        mat[:,k]=act[kk,:,ii:ksz+ii,jj:ksz+jj].reshape(-1)\n",
        "                        k +=1\n",
        "            #print(mat.shape)\n",
        "            mat_list.append(mat)\n",
        "        else:\n",
        "            act = net.act[act_key[i]].detach().cpu().numpy()\n",
        "            activation = act[0:bsz].transpose()\n",
        "            mat_list.append(activation)\n",
        "\n",
        "    print('-'*30)\n",
        "    print('Representation Matrix')\n",
        "    print('-'*30)\n",
        "    for i in range(len(mat_list)):\n",
        "        print ('Layer {} : {}'.format(i+1,mat_list[i].shape))\n",
        "    print('-'*30)\n",
        "    return mat_list\n",
        "\n",
        "\n",
        "def update_SGP (args, model, mat_list, threshold, task_id, feature_list=[], importance_list=[]):\n",
        "    print ('Threshold: ', threshold)\n",
        "    print(len(mat_list))\n",
        "    if not feature_list:\n",
        "        # After First Task\n",
        "        for i in range(len(mat_list)):\n",
        "            activation = mat_list[i]\n",
        "\n",
        "            U,S,Vh = np.linalg.svd(activation, full_matrices=False)\n",
        "            # criteria (Eq-1)\n",
        "            sval_total = (S**2).sum()\n",
        "            sval_ratio = (S**2)/sval_total\n",
        "\n",
        "            r = np.sum(np.cumsum(sval_ratio)<threshold[i]) #+1\n",
        "            # update GPM\n",
        "            feature_list.append(U[:,0:r])\n",
        "            # update importance (Eq-2)\n",
        "            importance = ((args.scale_coff+1)*S[0:r])/(args.scale_coff*S[0:r] + max(S[0:r]))\n",
        "            importance_list.append(importance)\n",
        "    else:\n",
        "        for i in range(len(mat_list)):\n",
        "            activation = mat_list[i]\n",
        "            U1,S1,Vh1=np.linalg.svd(activation, full_matrices=False)\n",
        "            sval_total = (S1**2).sum()\n",
        "            # Projected Representation (Eq-4)\n",
        "            act_proj = np.dot(np.dot(feature_list[i],feature_list[i].transpose()),activation)\n",
        "            r_old = feature_list[i].shape[1] # old GPM bases\n",
        "            Uc,Sc,Vhc = np.linalg.svd(act_proj, full_matrices=False)\n",
        "            importance_new_on_old = np.dot(np.dot(feature_list[i].transpose(),Uc[:,0:r_old])**2, Sc[0:r_old]**2) ## r_old no of elm s**2 fmt\n",
        "            importance_new_on_old = np.sqrt(importance_new_on_old)\n",
        "\n",
        "            act_hat = activation - act_proj\n",
        "            U,S,Vh = np.linalg.svd(act_hat, full_matrices=False)\n",
        "            # criteria (Eq-5)\n",
        "            sval_hat = (S**2).sum()\n",
        "            sval_ratio = (S**2)/sval_total\n",
        "            accumulated_sval = (sval_total-sval_hat)/sval_total\n",
        "\n",
        "            r = 0\n",
        "            for ii in range (sval_ratio.shape[0]):\n",
        "                if accumulated_sval < threshold[i]:\n",
        "                    accumulated_sval += sval_ratio[ii]\n",
        "                    r += 1\n",
        "                else:\n",
        "                    break\n",
        "            if r == 0:\n",
        "                print ('Skip Updating GPM for layer: {}'.format(i+1))\n",
        "                # update importances\n",
        "                importance = importance_new_on_old\n",
        "                importance = ((args.scale_coff+1)*importance)/(args.scale_coff*importance + max(importance))\n",
        "                importance [0:r_old] = np.clip(importance [0:r_old]+importance_list[i][0:r_old], 0, 1)\n",
        "                importance_list[i] = importance # update importance\n",
        "                continue\n",
        "            # update GPM\n",
        "            Ui=np.hstack((feature_list[i],U[:,0:r]))\n",
        "            # update importance\n",
        "            importance = np.hstack((importance_new_on_old,S[0:r]))\n",
        "            importance = ((args.scale_coff+1)*importance)/(args.scale_coff*importance + max(importance))\n",
        "            importance [0:r_old] = np.clip(importance [0:r_old]+importance_list[i][0:r_old], 0, 1)\n",
        "\n",
        "            if Ui.shape[1] > Ui.shape[0] :\n",
        "                feature_list[i]=Ui[:,0:Ui.shape[0]]\n",
        "                importance_list[i] = importance[0:Ui.shape[0]]\n",
        "            else:\n",
        "                feature_list[i]=Ui\n",
        "                importance_list[i] = importance\n",
        "\n",
        "    print('-'*40)\n",
        "    print('Gradient Constraints Summary')\n",
        "    print('-'*40)\n",
        "    for i in range(len(feature_list)):\n",
        "        print ('Layer {} : {}/{}'.format(i+1,feature_list[i].shape[1], feature_list[i].shape[0]))\n",
        "    print('-'*40)\n",
        "    return feature_list, importance_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8HkMPaT2lPPV"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "# import utils\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "cf100_dir = './data/'\n",
        "file_dir = './data/binary_cifar100'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OoSJYWKHlTMa"
      },
      "outputs": [],
      "source": [
        "def get(seed=0,pc_valid=0.10):\n",
        "    data={}\n",
        "    taskcla=[]\n",
        "    size=[3,32,32]\n",
        "\n",
        "    if not os.path.isdir(file_dir):\n",
        "        os.makedirs(file_dir)\n",
        "\n",
        "        mean=[x/255 for x in [125.3,123.0,113.9]]\n",
        "        std=[x/255 for x in [63.0,62.1,66.7]]\n",
        "\n",
        "        # CIFAR100\n",
        "        dat={}\n",
        "        dat['train']=datasets.CIFAR100(cf100_dir,train=True,download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean,std)]))\n",
        "        dat['test']=datasets.CIFAR100(cf100_dir,train=False,download=True,transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean,std)]))\n",
        "        # dat['train'] = datasets.CIFAR100(cf100_dir,train=True,download=False,transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        # dat['test']  = datasets.CIFAR100(cf100_dir,train=False,download=False,transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        for n in range(10):\n",
        "            data[n]={}\n",
        "            data[n]['name']='cifar100'\n",
        "            data[n]['ncla']=10\n",
        "            data[n]['train']={'x': [],'y': []}\n",
        "            data[n]['test']={'x': [],'y': []}\n",
        "        for s in ['train','test']:\n",
        "            loader=torch.utils.data.DataLoader(dat[s],batch_size=1,shuffle=False)\n",
        "            for image,target in loader:\n",
        "                n=target.numpy()[0]\n",
        "                nn=(n//10)\n",
        "                data[nn][s]['x'].append(image) # 255\n",
        "                data[nn][s]['y'].append(n%10)\n",
        "\n",
        "        # \"Unify\" and save\n",
        "        for t in data.keys():\n",
        "            for s in ['train','test']:\n",
        "                data[t][s]['x']=torch.stack(data[t][s]['x']).view(-1,size[0],size[1],size[2])\n",
        "                data[t][s]['y']=torch.LongTensor(np.array(data[t][s]['y'],dtype=int)).view(-1)\n",
        "                torch.save(data[t][s]['x'], os.path.join(os.path.expanduser(file_dir),'data'+str(t)+s+'x.bin'))\n",
        "                torch.save(data[t][s]['y'], os.path.join(os.path.expanduser(file_dir),'data'+str(t)+s+'y.bin'))\n",
        "\n",
        "    # Load binary files\n",
        "    data={}\n",
        "    # ids=list(shuffle(np.arange(5),random_state=seed))\n",
        "    ids=list(np.arange(10))\n",
        "    print('Task order =',ids)\n",
        "    for i in range(10):\n",
        "        data[i] = dict.fromkeys(['name','ncla','train','test'])\n",
        "        for s in ['train','test']:\n",
        "            data[i][s]={'x':[],'y':[]}\n",
        "            data[i][s]['x']=torch.load(os.path.join(os.path.expanduser(file_dir),'data'+str(ids[i])+s+'x.bin'))\n",
        "            data[i][s]['y']=torch.load(os.path.join(os.path.expanduser(file_dir),'data'+str(ids[i])+s+'y.bin'))\n",
        "        data[i]['ncla']=len(np.unique(data[i]['train']['y'].numpy()))\n",
        "        if data[i]['ncla']==2:\n",
        "            data[i]['name']='cifar10-'+str(ids[i])\n",
        "        else:\n",
        "            data[i]['name']='cifar100-'+str(ids[i])\n",
        "\n",
        "    # Validation\n",
        "    for t in data.keys():\n",
        "        r=np.arange(data[t]['train']['x'].size(0))\n",
        "        r=np.array(shuffle(r,random_state=seed),dtype=int)\n",
        "        nvalid=int(pc_valid*len(r))\n",
        "        ivalid=torch.LongTensor(r[:nvalid])\n",
        "        itrain=torch.LongTensor(r[nvalid:])\n",
        "        data[t]['valid']={}\n",
        "        data[t]['valid']['x']=data[t]['train']['x'][ivalid].clone()\n",
        "        data[t]['valid']['y']=data[t]['train']['y'][ivalid].clone()\n",
        "        data[t]['train']['x']=data[t]['train']['x'][itrain].clone()\n",
        "        data[t]['train']['y']=data[t]['train']['y'][itrain].clone()\n",
        "\n",
        "    # Others\n",
        "    n=0\n",
        "    for t in data.keys():\n",
        "        taskcla.append((t,data[t]['ncla']))\n",
        "        n+=data[t]['ncla']\n",
        "    data['ncla']=n\n",
        "\n",
        "    return data,taskcla,size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uetcz1T2lYjH"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    tstart=time.time()\n",
        "    ## Device Setting\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print (device)\n",
        "    ## setup seeds\n",
        "    # os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "    data,taskcla,inputsize=get(pc_valid=args.pc_valid)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    acc_matrix=np.zeros((10,10))\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    task_id = 0\n",
        "    task_list = []\n",
        "    for k,ncla in taskcla:\n",
        "        # specify threshold hyperparameter\n",
        "        threshold = np.array([args.gpm_eps] * 15) + task_id * np.array([args.gpm_eps_inc] * 15)\n",
        "\n",
        "        print('*'*100)\n",
        "        print('Task {:2d} ({:s})'.format(k,data[k]['name']))\n",
        "        print('*'*100)\n",
        "        xtrain=data[k]['train']['x']\n",
        "        ytrain=data[k]['train']['y']\n",
        "        xvalid=data[k]['valid']['x']\n",
        "        yvalid=data[k]['valid']['y']\n",
        "        xtest =data[k]['test']['x']\n",
        "        ytest =data[k]['test']['y']\n",
        "        task_list.append(k)\n",
        "\n",
        "        lr = args.lr\n",
        "        best_loss=np.inf\n",
        "        print ('-'*40)\n",
        "        print ('Task ID :{} | Learning Rate : {}'.format(task_id, lr))\n",
        "        print ('-'*40)\n",
        "\n",
        "\n",
        "        if task_id==0:\n",
        "            if args.md == \"alex\":\n",
        "                model = AlexNet(taskcla).to(device)\n",
        "            elif args.md == \"le\":\n",
        "                model = LeNet(taskcla).to(device)\n",
        "            elif args.md == \"vgg\":\n",
        "                model = VGG16(taskcla).to(device)\n",
        "\n",
        "            print ('Model parameters ---')\n",
        "            for k_t, (m, param) in enumerate(model.named_parameters()):\n",
        "                print (k_t,m,param.shape)\n",
        "            print ('-'*40)\n",
        "\n",
        "            best_model=get_model(model)\n",
        "            feature_list =[]\n",
        "            importance_list = []\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "            for epoch in range(1, args.n_epochs+1):\n",
        "                # Train\n",
        "                clock0=time.time()\n",
        "                train(args, model, device, xtrain, ytrain, optimizer, criterion, k)\n",
        "                clock1=time.time()\n",
        "                tr_loss,tr_acc = test(args, model, device, xtrain, ytrain,  criterion, k)\n",
        "                print('Epoch {:3d} | Train: loss={:.3f}, acc={:5.1f}% | time={:5.1f}ms |'.format(epoch,\\\n",
        "                                                            tr_loss,tr_acc, 1000*(clock1-clock0)),end='')\n",
        "                # Validate\n",
        "                valid_loss,valid_acc = test(args, model, device, xvalid, yvalid,  criterion, k)\n",
        "                print(' Valid: loss={:.3f}, acc={:5.1f}% |'.format(valid_loss, valid_acc),end='')\n",
        "                # Adapt lr\n",
        "                if valid_loss<best_loss:\n",
        "                    best_loss=valid_loss\n",
        "                    best_model=get_model(model)\n",
        "                    patience=args.lr_patience\n",
        "                    print(' *',end='')\n",
        "                else:\n",
        "                    patience-=1\n",
        "                    if patience<=0:\n",
        "                        lr/=args.lr_factor\n",
        "                        print(' lr={:.1e}'.format(lr),end='')\n",
        "                        if lr<args.lr_min:\n",
        "                            print()\n",
        "                            break\n",
        "                        patience=args.lr_patience\n",
        "                        adjust_learning_rate(optimizer, epoch, args)\n",
        "                print()\n",
        "            set_model_(model,best_model)\n",
        "            # Test\n",
        "            print ('-'*40)\n",
        "            test_loss, test_acc = test(args, model, device, xtest, ytest,  criterion, k)\n",
        "            print('Test: loss={:.3f} , acc={:5.1f}%'.format(test_loss,test_acc))\n",
        "            # Memory and Importance Update\n",
        "            mat_list = get_representation_matrix(model, device, xtrain, ytrain)\n",
        "            feature_list, importance_list = update_SGP(args, model, mat_list, threshold, task_id, feature_list, importance_list)\n",
        "\n",
        "        else:\n",
        "            optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
        "            feature_mat = []\n",
        "            # Projection Matrix Precomputation\n",
        "            for i in range(len(model.act)):\n",
        "                Uf=torch.Tensor(np.dot(feature_list[i],np.dot(np.diag(importance_list[i]),feature_list[i].transpose()))).to(device)\n",
        "                # print('Layer {} - Projection Matrix shape: {}'.format(i+1,Uf.shape))\n",
        "                Uf.requires_grad = False\n",
        "                feature_mat.append(Uf)\n",
        "            # print ('-'*40)\n",
        "            for epoch in range(1, args.n_epochs+1):\n",
        "                # Train\n",
        "                clock0=time.time()\n",
        "                train_projected(args, model,device,xtrain, ytrain,optimizer,criterion,feature_mat,k)\n",
        "                clock1=time.time()\n",
        "                tr_loss, tr_acc = test(args, model, device, xtrain, ytrain,criterion,k)\n",
        "                print('Epoch {:3d} | Train: loss={:.3f}, acc={:5.1f}% | time={:5.1f}ms |'.format(epoch,\\\n",
        "                                                        tr_loss, tr_acc, 1000*(clock1-clock0)),end='')\n",
        "                # Validate\n",
        "                valid_loss,valid_acc = test(args, model, device, xvalid, yvalid, criterion,k)\n",
        "                print(' Valid: loss={:.3f}, acc={:5.1f}% |'.format(valid_loss, valid_acc),end='')\n",
        "                # Adapt lr\n",
        "                if valid_loss<best_loss:\n",
        "                    best_loss=valid_loss\n",
        "                    best_model=get_model(model)\n",
        "                    patience=args.lr_patience\n",
        "                    print(' *',end='')\n",
        "                else:\n",
        "                    patience-=1\n",
        "                    if patience<=0:\n",
        "                        lr/=args.lr_factor\n",
        "                        print(' lr={:.1e}'.format(lr),end='')\n",
        "                        if lr<args.lr_min:\n",
        "                            print()\n",
        "                            break\n",
        "                        patience=args.lr_patience\n",
        "                        adjust_learning_rate(optimizer, epoch, args)\n",
        "                print()\n",
        "            set_model_(model,best_model)\n",
        "            # Test\n",
        "            test_loss, test_acc = test(args, model, device, xtest, ytest,  criterion,k)\n",
        "            print('Test: loss={:.3f} , acc={:5.1f}%'.format(test_loss,test_acc))\n",
        "            # Memory and Importance Update\n",
        "            mat_list = get_representation_matrix (model, device, xtrain, ytrain)\n",
        "            feature_list, importance_list = update_SGP (args, model, mat_list, threshold, task_id, feature_list, importance_list)\n",
        "\n",
        "        # save accuracy\n",
        "        jj = 0\n",
        "        for ii in np.array(task_list)[0:task_id+1]:\n",
        "            xtest =data[ii]['test']['x']\n",
        "            ytest =data[ii]['test']['y']\n",
        "            _, acc_matrix[task_id,jj] = test(args, model, device, xtest, ytest,criterion,ii)\n",
        "            jj +=1\n",
        "        print('Accuracies =')\n",
        "        for i_a in range(task_id+1):\n",
        "            print('\\t',end='')\n",
        "            for j_a in range(acc_matrix.shape[1]):\n",
        "                print('{:5.1f}% '.format(acc_matrix[i_a,j_a]),end='')\n",
        "            print()\n",
        "        # update task id\n",
        "        task_id +=1\n",
        "    print('-'*50)\n",
        "\n",
        "    # Simulation Results\n",
        "    # print ('Task Order : {}'.format(np.array(task_list)))\n",
        "    # print(\"Configs: seed: {} | lr: {} | gpm_eps: {} | gpm_eps_inc: {} | scale_coff: {}\".format(args.seed,args.lr,args.gpm_eps,args.gpm_eps_inc,args.scale_coff))\n",
        "    print ('Final Avg Accuracy: {:5.2f}%'.format(acc_matrix[-1].mean()))\n",
        "    bwt=np.mean((acc_matrix[-1]-np.diag(acc_matrix))[:-1])\n",
        "    print ('Backward transfer: {:5.2f}%'.format(bwt))\n",
        "    print('[Elapsed time = {:.1f} ms]'.format((time.time()-tstart)*1000))\n",
        "    print('-'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pno2YsOl__Q",
        "outputId": "39368bb7-7fe1-49b5-e2a9-baf29eeb3bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "Task order = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "****************************************************************************************************\n",
            "Task  0 (cifar100-0)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :0 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Model parameters ---\n",
            "0 conv1.weight torch.Size([64, 3, 4, 4])\n",
            "1 bn1.weight torch.Size([64])\n",
            "2 bn1.bias torch.Size([64])\n",
            "3 conv2.weight torch.Size([128, 64, 3, 3])\n",
            "4 bn2.weight torch.Size([128])\n",
            "5 bn2.bias torch.Size([128])\n",
            "6 conv3.weight torch.Size([256, 128, 2, 2])\n",
            "7 bn3.weight torch.Size([256])\n",
            "8 bn3.bias torch.Size([256])\n",
            "9 fc1.weight torch.Size([2048, 1024])\n",
            "10 bn4.weight torch.Size([2048])\n",
            "11 bn4.bias torch.Size([2048])\n",
            "12 fc2.weight torch.Size([2048, 2048])\n",
            "13 bn5.weight torch.Size([2048])\n",
            "14 bn5.bias torch.Size([2048])\n",
            "15 fc3.0.weight torch.Size([10, 2048])\n",
            "16 fc3.1.weight torch.Size([10, 2048])\n",
            "17 fc3.2.weight torch.Size([10, 2048])\n",
            "18 fc3.3.weight torch.Size([10, 2048])\n",
            "19 fc3.4.weight torch.Size([10, 2048])\n",
            "20 fc3.5.weight torch.Size([10, 2048])\n",
            "21 fc3.6.weight torch.Size([10, 2048])\n",
            "22 fc3.7.weight torch.Size([10, 2048])\n",
            "23 fc3.8.weight torch.Size([10, 2048])\n",
            "24 fc3.9.weight torch.Size([10, 2048])\n",
            "----------------------------------------\n",
            "cpu\n",
            "Epoch   1 | Train: loss=3.392, acc= 30.3% | time=7483.4ms | Valid: loss=3.675, acc= 26.8% | *\n",
            "----------------------------------------\n",
            "Test: loss=3.491 , acc= 29.5%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.97 0.97\n",
            " 0.97]\n",
            "5\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 9/48\n",
            "Layer 2 : 82/576\n",
            "Layer 3 : 82/512\n",
            "Layer 4 : 14/1024\n",
            "Layer 5 : 16/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  1 (cifar100-1)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :1 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=2.647, acc= 36.1% | time=8810.1ms | Valid: loss=2.650, acc= 36.0% | *\n",
            "Test: loss=2.807 , acc= 35.1%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973 0.973\n",
            " 0.973 0.973 0.973]\n",
            "5\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 11/48\n",
            "Layer 2 : 127/576\n",
            "Layer 3 : 152/512\n",
            "Layer 4 : 28/1024\n",
            "Layer 5 : 32/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  2 (cifar100-2)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :2 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=1.784, acc= 52.5% | time=8975.7ms | Valid: loss=2.084, acc= 49.2% | *\n",
            "Test: loss=1.942 , acc= 48.6%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.976 0.976 0.976 0.976 0.976 0.976 0.976 0.976 0.976 0.976 0.976 0.976\n",
            " 0.976 0.976 0.976]\n",
            "5\n",
            "Skip Updating GPM for layer: 1\n",
            "Skip Updating GPM for layer: 2\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 11/48\n",
            "Layer 2 : 127/576\n",
            "Layer 3 : 187/512\n",
            "Layer 4 : 42/1024\n",
            "Layer 5 : 49/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  3 (cifar100-3)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :3 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=2.173, acc= 44.0% | time=8717.4ms | Valid: loss=2.436, acc= 44.8% | *\n",
            "Test: loss=2.357 , acc= 41.0%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.979 0.979 0.979 0.979 0.979 0.979 0.979 0.979 0.979 0.979 0.979 0.979\n",
            " 0.979 0.979 0.979]\n",
            "5\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 12/48\n",
            "Layer 2 : 148/576\n",
            "Layer 3 : 224/512\n",
            "Layer 4 : 57/1024\n",
            "Layer 5 : 66/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.9%  33.9%  49.2%  41.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  4 (cifar100-4)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :4 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=1.630, acc= 53.5% | time=8931.6ms | Valid: loss=1.827, acc= 48.8% | *\n",
            "Test: loss=1.747 , acc= 51.5%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.982 0.982 0.982 0.982 0.982 0.982 0.982 0.982 0.982 0.982 0.982 0.982\n",
            " 0.982 0.982 0.982]\n",
            "5\n",
            "Skip Updating GPM for layer: 1\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 12/48\n",
            "Layer 2 : 172/576\n",
            "Layer 3 : 258/512\n",
            "Layer 4 : 72/1024\n",
            "Layer 5 : 83/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.9%  33.9%  49.2%  41.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.9%  34.2%  49.8%  42.0%  52.8%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  5 (cifar100-5)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :5 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=1.558, acc= 50.4% | time=8815.5ms | Valid: loss=1.846, acc= 42.8% | *\n",
            "Test: loss=1.672 , acc= 45.7%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985 0.985\n",
            " 0.985 0.985 0.985]\n",
            "5\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 13/48\n",
            "Layer 2 : 198/576\n",
            "Layer 3 : 286/512\n",
            "Layer 4 : 87/1024\n",
            "Layer 5 : 101/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.9%  33.9%  49.2%  41.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.9%  34.2%  49.8%  42.0%  52.8%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 31.0%  35.0%  49.5%  41.0%  52.3%  45.6%   0.0%   0.0%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  6 (cifar100-6)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :6 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=1.636, acc= 47.6% | time=10358.0ms | Valid: loss=1.825, acc= 42.4% | *\n",
            "Test: loss=1.834 , acc= 46.5%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.988 0.988 0.988 0.988 0.988 0.988 0.988 0.988 0.988 0.988 0.988 0.988\n",
            " 0.988 0.988 0.988]\n",
            "5\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 17/48\n",
            "Layer 2 : 233/576\n",
            "Layer 3 : 319/512\n",
            "Layer 4 : 103/1024\n",
            "Layer 5 : 119/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.9%  33.9%  49.2%  41.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.9%  34.2%  49.8%  42.0%  52.8%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 31.0%  35.0%  49.5%  41.0%  52.3%  45.6%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.4%  35.6%  49.5%  41.9%  54.3%  46.3%  46.5%   0.0%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  7 (cifar100-7)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :7 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=1.460, acc= 52.1% | time=11039.1ms | Valid: loss=1.434, acc= 52.8% | *\n",
            "Test: loss=1.496 , acc= 52.4%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.991 0.991 0.991 0.991 0.991 0.991 0.991 0.991 0.991 0.991 0.991 0.991\n",
            " 0.991 0.991 0.991]\n",
            "5\n",
            "Skip Updating GPM for layer: 1\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 17/48\n",
            "Layer 2 : 242/576\n",
            "Layer 3 : 340/512\n",
            "Layer 4 : 118/1024\n",
            "Layer 5 : 136/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.9%  33.9%  49.2%  41.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.9%  34.2%  49.8%  42.0%  52.8%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 31.0%  35.0%  49.5%  41.0%  52.3%  45.6%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.4%  35.6%  49.5%  41.9%  54.3%  46.3%  46.5%   0.0%   0.0%   0.0% \n",
            "\t 30.8%  35.0%  49.6%  41.9%  52.3%  46.4%  47.5%  49.4%   0.0%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  8 (cifar100-8)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :8 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=2.174, acc= 40.7% | time=12637.3ms | Valid: loss=2.385, acc= 37.2% | *\n",
            "Test: loss=2.332 , acc= 39.9%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.994 0.994 0.994 0.994 0.994 0.994 0.994 0.994 0.994 0.994 0.994 0.994\n",
            " 0.994 0.994 0.994]\n",
            "5\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 20/48\n",
            "Layer 2 : 306/576\n",
            "Layer 3 : 386/512\n",
            "Layer 4 : 136/1024\n",
            "Layer 5 : 155/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.9%  33.9%  49.2%  41.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.9%  34.2%  49.8%  42.0%  52.8%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 31.0%  35.0%  49.5%  41.0%  52.3%  45.6%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.4%  35.6%  49.5%  41.9%  54.3%  46.3%  46.5%   0.0%   0.0%   0.0% \n",
            "\t 30.8%  35.0%  49.6%  41.9%  52.3%  46.4%  47.5%  49.4%   0.0%   0.0% \n",
            "\t 30.9%  36.0%  49.2%  41.3%  54.3%  47.7%  46.3%  51.6%  38.7%   0.0% \n",
            "****************************************************************************************************\n",
            "Task  9 (cifar100-9)\n",
            "****************************************************************************************************\n",
            "----------------------------------------\n",
            "Task ID :9 | Learning Rate : 0.05\n",
            "----------------------------------------\n",
            "Epoch   1 | Train: loss=1.418, acc= 56.0% | time=12362.5ms | Valid: loss=1.496, acc= 52.4% | *\n",
            "Test: loss=1.544 , acc= 52.3%\n",
            "------------------------------\n",
            "Representation Matrix\n",
            "------------------------------\n",
            "Layer 1 : (48, 16820)\n",
            "Layer 2 : (576, 2880)\n",
            "Layer 3 : (512, 500)\n",
            "Layer 4 : (1024, 20)\n",
            "Layer 5 : (2048, 20)\n",
            "------------------------------\n",
            "Threshold:  [0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997 0.997\n",
            " 0.997 0.997 0.997]\n",
            "5\n",
            "----------------------------------------\n",
            "Gradient Constraints Summary\n",
            "----------------------------------------\n",
            "Layer 1 : 23/48\n",
            "Layer 2 : 385/576\n",
            "Layer 3 : 437/512\n",
            "Layer 4 : 155/1024\n",
            "Layer 5 : 175/2048\n",
            "----------------------------------------\n",
            "Accuracies =\n",
            "\t 29.8%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.4%  33.4%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.2%  34.5%  49.1%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 29.9%  33.9%  49.2%  41.0%   0.0%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.9%  34.2%  49.8%  42.0%  52.8%   0.0%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 31.0%  35.0%  49.5%  41.0%  52.3%  45.6%   0.0%   0.0%   0.0%   0.0% \n",
            "\t 30.4%  35.6%  49.5%  41.9%  54.3%  46.3%  46.5%   0.0%   0.0%   0.0% \n",
            "\t 30.8%  35.0%  49.6%  41.9%  52.3%  46.4%  47.5%  49.4%   0.0%   0.0% \n",
            "\t 30.9%  36.0%  49.2%  41.3%  54.3%  47.7%  46.3%  51.6%  38.7%   0.0% \n",
            "\t 30.5%  36.1%  49.2%  42.5%  54.0%  46.7%  48.8%  50.5%  38.9%  51.4% \n",
            "--------------------------------------------------\n",
            "Final Avg Accuracy: 44.86%\n",
            "Backward transfer:  1.21%\n",
            "[Elapsed time = 200203.7 ms]\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class arg_class():\n",
        "    def __init__(self,batch_size_train=64, batch_size_test=64, n_epochs=1, seed=5, pc_valid=0.05, lr=0.05, momentum=0.9, lr_min=5e-05, lr_patience=6, lr_factor=2, scale_coff=10, gpm_eps=0.97, gpm_eps_inc=0.003,mod=\"alex\"):\n",
        "        self.batch_size_train=batch_size_train\n",
        "        self.batch_size_test=batch_size_test\n",
        "        self.n_epochs=n_epochs\n",
        "        self.seed=seed\n",
        "        self.pc_valid=pc_valid\n",
        "        self.lr=lr\n",
        "        self.momentum=momentum\n",
        "        self.lr_min=lr_min\n",
        "        self.lr_patience=lr_patience\n",
        "        self.lr_factor=lr_factor\n",
        "        self.scale_coff=scale_coff\n",
        "        self.gpm_eps=gpm_eps\n",
        "        self.gpm_eps_inc=gpm_eps_inc\n",
        "        self.md = mod\n",
        "\n",
        "args = arg_class(mod = \"alex\")\n",
        "\n",
        "main(args)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
